# 重复数据删除

重复数据删除在维基百科上的定义为“一种 可粗粒度去除冗余数据的特殊数据压缩技术”，开宗明义地解释了重复数 据删除和数据压缩之间的联系。重复数据删除（DEDUPE）在存储备份系统中有很重要的应用，有 利于更高效地利用存储空间。

通俗地讲，数据压缩一般通过字符串或比特级的操作来<mark style="color:blue;">**去除冗余数据**</mark>，然而重复数据删除判断数据冗余的粒度较大，一般是文件级别或块级别的匹配，其目标是达到性能和去重复比例的平衡。

{% hint style="info" %}
<mark style="color:red;">**为何需要进行重复数据删除呢？**</mark>

数据的快速增长是对数据中心最大的挑战，爆炸式的数据增长会消耗巨大的存储空间，这会迫使数据提供商去购买更多的存储，然而即使这样却未必能赶上数据的增长速度。这 样的现实迫使我们去考虑一些问题：产生的数据是不是都被生产系统循环使用？如果不是，是不是可以把这些数据放到廉价的存储系统中？怎么让数据备份消耗的存储更低？怎么让备份的时间更短？数据备份后， 可以保存的时间有多久（物理介质原因）？备份后的数据能不能正常取出？
{% endhint %}

在实际情况中，和生产系统连接的备份系统一般每隔一段时间就会 对生产系统做一次主备份，即备份生产系统中的所有内容。在两次主备 份之间有很多增量式备份，且生产系统和备份系统的工作时间一般来讲 是互斥的。当然如果生产系统需要持续运行，那么备份系统则需要在生 产系统相对空闲的时间来工作。也就是说，备份系统的工作时间是有限 制的，一般这个时间被称为备份窗口。需要被备份的数据必须在这个时 间全部被迁移到备份系统中，这就对备份系统的吞吐率提出了要求。

那么怎么提高备份系统的吞吐率呢？纯粹更换硬件是一种方法。例 如，10年前，主流的备份系统用的是磁带，其局限性在于吞吐率不够而 且只支持顺序读/写。为满足吞吐率方面的需求，主流的备份系统的后端 存储开始采用磁盘。

用于备份的数据，或者多次备份过来的数据可能是相似的，如果我 们不是机械化地去备份这些数据，而是有意识地识别其中冗余的数据， 然后对相同的数据只备份一份或少数几份（出于数据可靠性方面的考 虑），那么这种方法不仅减少了备份系统所需要的容量，还减少了备份 时对带宽的使用。举一个现实的例子，邮件系统中有很多群发邮件，其 内容相同，特别是附件。试问邮件备份系统中需要将用户的相同附件都 保存一份吗？答案显然是不必要的。邮件系统的例子体现出重复数据删 除的作用，不过邮件系统所用的重复数据删除比较简单，主要针对的是 相同的文件。

简言之，文件级别的重复数据删除有很大的局限性。最浅显的问题 就是：如果文件内容只是略微有些不同，那要怎样进行数据去重呢？换 句话说，文件级别的粒度太大，而粒度太小就变成了常用数据压缩技术，也不太合适。所以我们需要的是粒度适中的，如大小平均在8KB左 右（经验值）。这样的重复数据删除能让空间（数据去重比率）和时间 （备份性能）达到一个最佳的平衡点。

综上所述，重复数据删除一般用于备份系统中（或二级存储）。衡 量一个应用重复数据删除的备份系统是不是优秀，有以下几个主要特 征。&#x20;

* 数据去重复率。数据去重复率越大越能减少备份系统存储方面的 压力，同时在二次或多次数据备份的时候，越能减少对网络带宽的使 用。&#x20;
* 吞吐率。使用了数据去重技术后，备份时间是否显著缩短。根据 前面所说的，对于很多生产系统来讲，备份窗口时间有限。如果数据吞 吐率不高，即使再高的数据压缩比，也很难被采用。
* 数据的可靠性。这是指经过重复数据删除处理后的数据在进行灾 难恢复时，数据能否被正常恢复，因为去重后的数据已经不是原来的数 据了。这一点往往被外行人忽略，但是一个好的数据备份厂商，一定会 重视这个问题。试想如果去重后的数据在某些情况下不能被正常恢复， 那又何必应用重复数据删除做数据备份呢？&#x20;
* 备份过程的安全性。对于企业内部或私有云，基本上默认在数据 备份过程中不会出现安全方面的问题，如数据窃取。但是如果将重复数 据删除技术作为一个由第三方提供的服务，那么安全问题就需要被重 视。
